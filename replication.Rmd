---
title: "Understanding the Role of Dynamic and Static Analysis for Mining Android Sandboxes" 
author: "da Costa et al."
date: "March, 2021"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd(".")
require(sqldf)
require(reshape2)
require(xtable)
require(kableExtra)
require(ggplot2)
require(VennDiagram)
require(ggvenn)
```

## Replication Package

This is the replication package for the paper *Mining Android Sandboxes Using Dynamic and Static Analysis: a
Replication Study*. 

### Authors

   * Francisco Handrick da Costa
   * Ismael Medeiros
   * Thales Menezes
   * João Victor da Silva
   * Ingrid Lorraine da Silva
   * Rodrigo Bonifácio
   * Krishna Narasimhanb
   * Márcio Ribeiro
  
### Abstract

The popularization of the Android platform and the growing number of Android applications (apps) that manage sensitive data turned the Android ecosystem into an attractive target for malicious software. For this reason, researchers and practitioners have investigated new approaches to address Android's security issues, including techniques that leverage dynamic analysis to mine Android sandboxes. The mining sandbox approach consists in running dynamic analysis tools on a benign version of an Android app. This exploratory phase records all calls to sensitive APIs. Later, we can use this information to (a) prevent calls to other sensitive APIs (those not recorded in the exploratory phase) or (b) run the dynamic analysis tools again in a different version of the app. During this second execution of the fuzzing tools, a warning of possible malicious behavior is raised whenever the new version of the app calls a sensitive API not recorded in the exploratory phase.

The use of a mining sandbox approach is an effective technique for Android malware analysis, as previous research works revealed. Particularly, existing reports present an accuracy of almost 70% in the identification of malicious behavior using dynamic analysis tools to mine android sandboxes. However, although the use of dynamic analysis for mining Android sandboxes has been investigated before, little is known about the potential benefits of combining static
analysis with a mining sandbox approach for identifying malicious behavior.
Accordingly, in this paper we present the results of two studies that investigate the impact of using static analysis to complement the performance of existing dynamic analysis tools tailored for mining Android sandboxes, in the task of identifying malicious behavior.

In the first study we conduct a non-exact replication of a previous study (hereafter BLL-Study) that compares the performance of test case generation tools for mining Android sandboxes. Differently from the original work, here we isolate the effect of an independent  static analysis component (DroidFax) they used to instrument the Android apps in their experiments. This decision was motivated by the fact that DroidFax could have influenced the efficacy of the dynamic analyses tools positively---through the execution of specific static analysis algorithms DroidFax also implements. In our second study, we carried out a new experiment to investigate the efficacy of taint analysis algorithms to complement the mining sandbox approach previously used to identify malicious behavior. To this end, we executed the FlowDroid tool to mine the source-sink flows from benign/malign pairs of Android apps used in a previous research work.

Our study brings several findings. For instance, the first study reveals that DroidFax alone (static analysis) can detect 43.75% of the malwares in the BLL-Study dataset, contributing substantially in the the performance of the dynamic analysis tools in the BLL-Study. The results of the second study show that taint analysis is also practical to complement the mining sandboxes approach, with a performance similar to that reached by dynamic analysis tools.

## First Study

To replicate our first study we used our benchmark tool (DroidXP). For the process its necessary proceed with the steps below

<b>Step 1</b>

Clone DroidXP Repository and install DroidXP:
```
git clone https://github.com/droidxp/benchmark
```
Clone repository to configure benchmark
```
https://github.com/droidxp/benchmark-vm
```


After clone the repositories, create a folder called 'script' at home directory,
and copy the 3 file from benchmark-vm/ubuntu_20.04_python3_x86/ <b>to</b> ~/script folder and execute. 
```
./config.sh
```
After these steps, DroiXP it installed and configured correctly with all tools used at first study.


<b>Step 2</b> - Download pair apps (Malign/Benign) for analysis

All apps used in this research are listed in [LargeI.csv](https://github.com/droidxp/benchmark/blob/develop/data/input/LargeI.csv)
and [SmallE.csv](https://github.com/droidxp/benchmark/blob/develop/data/input/SmallE.csv) files. They can be downloaded using the Python script
[getApps.py](https://github.com/droidxp/benchmark/blob/develop/data/input/getApps.py). At our experiments (First and Second), we used pairs apps from the file [LargeI.csv](https://github.com/droidxp/benchmark/blob/develop/data/input/LargeI.csv), except for the following apps: app-25, app-32, app-36, app-41, app-88 and app-93 (96 pairs of Android apps)

These apps come from the [Androzoo](https://androzoo.uni.lu/) repository, so it is necessary
to acquire an access key and configure it in the Python script:

```py
key="Insert here your key"
```

To change Apps set to download, just configure the Python script.

```py
with open('LargeI.csv') as csvfile:
```
or 
```py
with open('SmallE.csv') as csvfile:
```


These apps MUST be stored in the `benchmark/data/input/` folder.

> We recommend using the script to download the apps because it formats the name of the
> files to organize in malign and benign apps.

<b>Step 3</b> - Run DroidXP

To run our experiment disabling static analysis, for 3 minutes (120 seconds), for 3 times, with 4 pre-configured test generator tools, we execute the follow command:

```py
python3 main.py -tools monkey droidbot droidmate humanoid -t 120 -r 3 --disable-static
```
To run fake tool joker, for 3 minutes (120 seconds), for 3 times, using static analysis component (DroidFax)

```py
python3 main.py -tools joker -t 120 -r 3
```

The results will be at: `benchmark/results/<timestamp>/report/` folder.

Example: benchmark/results/20210220212608/report/


<b>Step 4</b> - Generate results at csv file

To generate the results at csv format, we must execute the follow command:

```py
python3 main.py --generate-output <timestamp> --output csv
```

Where `<timestamp>` is the folder where the data to generate csv file are.
Example:
```py
python3 main.py --generate-output 20210220212608 --output csv
```


### Data Analysis (First Study)

#### Load the Datasets

```{r}
ds <- read.csv("results.csv", head=T, sep=",")
```


#### Summary of the Results (**Table 1**)

````{r}
staticAnalysis <- sqldf("select tool, count(distinct app) totalWith
                        from ds 
                        where malware = 1 and static = 1 and tool != 'flowdroid'
                        group by tool")

withoutStaticAnalysis <-  sqldf("select tool, count(distinct app) totalWithout
                                  from ds 
                                  where malware = 1 and static = 0
                                  group by tool")

res <- merge(staticAnalysis, withoutStaticAnalysis, all.x=T)

res[is.na(res)] <- 0
res["Improvement"] = (res$totalWith  - res$totalWithout) * 100 / res$totalWith 

res <- res[order(-res$totalWith),]

colnames(res) <- c("Tool", "Using Static Analysis",	"Without Using Static Analysis", "Improvement")
res %>% kable("html") 
```


#### Compare the performance of the different tools (with static analysis)

```{r, venn-plot-s1}
sets <- sqldf("select tool, app, count(*) total 
                  from ds 
                  where malware = 1 and static = 1
                  group by tool, app")

monkey = sets[sets$tool=="monkey", c("app")]
humanoid = sets[sets$tool=="humanoid", c("app")]
droidmate = sets[sets$tool=="droidmate", c("app")]
droidbot = sets[sets$tool=="droidbot", c("app")]
joker = sets[sets$tool=="joke", c("app")]

x <- list(
  Monkey = monkey,
  Humanoid = humanoid, 
  DroidMate = droidmate,
  DroidBot = droidbot
)

ggvenn(
 x,
 fill_color = c("#0073C2FF", "#EFC000FF", "#868686FF", "#CD534CFF","#CD534CAA"),
   stroke_size = 0.5, set_name_size = 4, text_size=2
)
```


## Second Study

To replicate our second study we used FlowDroid version 2.8 which can be got in [secure-software-engineering/FlowDroid](https://github.com/secure-software-engineering/FlowDroid/releases/tag/v2.8) repository at GitHub. We recommend using the `soot-infoflow-cmd-jar-with-dependencies.jar` file because already have all FlowDroid dependencies, and is easy to configure with Maven using the command below. For more details access the [git repository](https://github.com/joao-victor-silva/droidxp-taint-analysis-tests).

```
mvn install:install-file -Dfile=<path-to-soot-infoflow-cmd-with-dependencies.jar> \
  -DgroupId=de.tud.sse \
  -DartifactId=soot-infoflow-cmd \
  -Dversion=2.8 \
  -Dpackaging=jar

```

At second study to run the test cases, you will need the same pairs apps used in the first study. Check step 2 from the first study to get information of how download the pairs apps.

### Data Analysis (Second Study)

#### Load the Datasets
```{r}
sets <- sqldf("select tool, app 
                  from ds 
                  where malware = 1 
                    and (tool='flowdroid' or tool = 'joke')")

notDetected <- sqldf("select distinct app from ds where 
                      app not in (select app from sets)")
```


#### Compare the performance of FlowDroid and DroidFax (Joker tool)


```{r, venn-plot-s2}


flowdroid = sets[sets$tool=="flowdroid", c("app")]
droidfax = sets[sets$tool=="joke", c("app")]
notDetected = sets[, c("app")]

x <- list(
  FlowDroid = flowdroid,
  DroidFax = droidfax
)

ggvenn(
 x,
 fill_color = c("#0073C2FF", "#EFC000FF"),
   stroke_size = 0.5, set_name_size = 4, text_size=2.5
)

```

## Malware dataset (2 Studies)

```{r echo=FALSE}
apps <- sqldf("select app, tool, sum(malware) total
               from ds 
               group by app, tool")

apps["detected"] <- ifelse(apps$total > 0, "x", "")
apps <- apps[,c("app", "tool", "detected")]
apps <- dcast(apps, app~tool)
apps["diff"] <- paste0("[diff/", apps$app, ".txt]", "(", "./diff/dif", gsub("-", "_", toupper(apps$app)), ".txt", ")")


info <- read.csv("allInfoApps.csv", head=T, sep=",")

info <- info[, c("id", "pkg_name", "hash")]
colnames(info) <- c("app", "name")
apps <- merge(apps, info)
```



```{r echo = FALSE}
apps %>%
  kable("html") %>%
  kable_styling(font_size = 12)  %>% 
  kable_styling("striped") %>% 
  scroll_box(width = "100%")

```

## FlowDroid Source-Sink detected and execution time


```{r echo=FALSE}
   flow <- read.csv("flowDroidResult.csv", head=T, sep=",")
   colnames(flow) <- c("app", "Detected", "Source-Sink (Benign)", "Source-Sink (Malicious)", "Total Runtime")

   flow["report"] <- paste0("[dataFlowDroid/", flow$app, ".txt]", "(", "./dataFlowDroid/", flow$app, ".txt", ")")
  
```


```{r echo = FALSE}
flow %>%
  kable("html") %>%
  kable_styling(font_size = 12)  %>% 
  kable_styling("striped") %>% 
  scroll_box(width = "100%")

```




